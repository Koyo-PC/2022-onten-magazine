---
title: 3000円でVRトラッカーを自作してみた
author: ZOI
---

どうもこんにちは、ZOIです。
みなさん、「VR」って知ってますか?
あのゴーグルを被るやつです。
実はVRってゴーグルをかぶって終わりじゃないんです。
最近のVRって結構進化していて、顔の向きに合わせて映像の向きを変えるだけじゃなくて、例えば手の位置や握り方を判別してくれたり、実際に歩いたらVR上でも前に進んだりします。
それ以外にも、自分の体の形をVRに反映してくれるような機械もあります。
この記事では「VRに自分の動きを反映する」ような機械を自分で作ってみたので、それの解説をしてみようと思います。

## 1. VRトラッカーって何?

まず初めに、記事のタイトルにもある「VRトラッカー」とはなんなのかから説明していきます。
トラッカーっていうのは下の画像みたいなもので(形は様々ですが)、基本的には「部屋の中でトラッカーがどこにあるのかわかるやつ」です。
![VIVE Tracker](https://www.vive.com/media/filer_public/fed-assets/tracker3/images/introducing-tracker-1366.jpg)
規模が小さいGPSみたいなものです。

普通にVRをしているだけでもヘッドセットの位置やコントローラの位置は把握してくれるんですが、これを手や足につけることで手や足の動きもVRに映し出すことができるというわけです。
VRで綺麗に座る・歩くことができ、寝ることすらできるようになります。

じゃあこれを買えばいいじゃん、という話なんですが、そういうわけにもいかないんです。
その理由は、これの値段にあります。なんとこの機械、一つで**17,500円**します。(2022/3/13)
さらに、十分にトラッキングするには少なくとも腰と両足で3ついるので、52,500円です。
僕が使ってるOculus Quest 2ならもう一つ買ってお釣りがくるレベルです。
ちょっと遊ぶには高すぎるんですよね。

僕以外にも同じようなことを考えている人がいるらしく、もっと安く体のトラッキングができる方法もいろいろ作られています。

例えば、

- HaritoraX (¥27,900)
- Uni-motion (予約販売のみ)
- Tundra Tracker (開発中)
- Kinect (¥15,000弱 Xbox用のものを流用)

などです。

ですが、体の形を近似するだけのデバイスならもっと安く作れるのでは? ということで、VRトラッカーの最安値を目指して自分で作ってみることにしました。

## 2. 色々なトラッカーの仕組み

まず、どうやって3次元の座標を作りだすか考えないといけません。

### 2a. GPS

とりあえず初めに思いつくのはGPSを使ってなんとかすることですが、GPSは(民生用のものでは)かなりの誤差(10mくらい)が出てしまいます。
腕の動きは数cmの精度で測る必要があるので、ちょっと厳しいです。

### 2b. 自作GPS

GPSで地球上の位置を特定できるなら、部屋1つ分の大きさのGPSを作ればかなりの精度で測定できるのでは...?

と思ったんですが、少し調べてみたところ、光の速度を利用しているとか、人工衛星の運動での相対性理論や重力場によって時間の流れが変わるとか、そのレベルの厳密な計算をしてるらしいので、ちょっと自宅で実現するのは無理そうです。

### 2c. VIVE Trackerの方法

さっき紹介したVIVEトラッカーに使われてる方法を見てみましょう。
あのトラッカーを使う時には、ベースステーションっていう機械を2個以上置く必要があります。
そこから赤外線を出して、あたった時間のズレでトラッカーの位置を判断するようになっています。

参考: <https://youtu.be/J54dotTt7k0>

ただこの方法、できるだけリアルタイムで表示するために、数十〜数百μs間隔で信号を送っているらしいです。この時間での制御はコスト的に難しそうで、かつ検出角度が1度ずれるだけでもかなりの誤差になるので、無理そうです。

### 2d. カメラでAIに持ち込む方法

ちょっと変わったやり方として、「全身をカメラに納め、AIを使って姿勢を推定する」という方法があります。
この方法では、スマホカメラを使う仕組みにすれば何も機材が要らないので、アプリ開発費を無視すれば無料で作ることができます。
ただ、この方法を考えている人はいっぱいいて、個人ではIT大手に勝てそうにないので、今回は使わないです。
(あとAIをあんまり信じていないというのも大きいですw)

### 2e. 加速度センサの利用

このセンサーで得たい情報は、「センサをつけた場所の部屋での位置」だけです。つまり、最初の位置さえ記録しておけば、そこからの加速度を2回積分することでどれだけ動いたかがわかるということです。

このためには加速度センサとジャイロセンサ(回転速度を測る)を使えばいいんですが、速度ではなく加速度を使っているので、僕の技術で誤差をどれだけ減らせるかというのが心配です。ちょっとミスると、腕が永久に向こうへ移動していくトラッカーができてしまいます...
あと、小型のジャイロセンサとして有名な`ENC-03RC/D`を使うと、3軸の回転を測るために3つのセンサを90度ずつずらして配置する必要があります。部品1つ1つは小さいのですが、立体的に組むとなると結構な分厚さになるのでは...?という感じです。(それでもVIVEより小さいと思いますが)

## 3. この記事で作るトラッカーの仕組み

じゃあどうやって作るんだという話なんですが、ここで人間の体で動くのはどこなのか、ということを考えてみましょう。
この話題はゲーム制作の現場ではよく考えれらていて(キャラクターをできるだけリアルに動かすためです)、「Humanoidボーン」という形である程度規格化されています。
特に、VRChatに使えるボーンというのはBoothにいっぱい転がっています。以下のような感じです。

![Boothのスクリーンショット](./Assets/Image/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202022-03-13%2012.16.30.png)

さて、これで何が言いたかったかというと、「たったこれだけのボーンの角度を変えることで人の姿勢が作れる」ということです。例えば足の関節以外のところは曲がりません。

つまり、それぞれの骨の角度さえわかれば、体全体の姿勢がわかるのではないか?ということです。

ということで、この方法で作っていきます!

## 4. いざ制作

### 4a. 部品の選定

まず、どのようにして角度を取るかです。
一般的にはジャイロセンサを使うのですが、それでは[方法2e](#2e-加速度センサの利用)と変わりません。

重力に対しての傾きを検出するセンサはないのかとも考えましたが、よく考えるとジャンプなどで大きな加速度がかかった時にズレそうです。

そこで今回は、加速度がかかってもズレず、周りが塞がっていても検出できそうなセンサとして、`地磁気センサ`を使うことにしました。

後でまた問題になりますが、地磁気センサは結構扱いが難しいです。まあそこはソフトウェアでなんとかすることにしましょう。

というわけで、今回使う材料は、ざっと以下の通りです!

- 地磁気センサ GY-273 (チップ : HMC5883L)
- Arduino Pro Mini (センサの読み取り・PCへの送信用)
- ユニバーサル基板 (配線用)
- 導線 (センサ・マイコンの通信用)
- 面ファスナー (体への固定用)

### 4b. 購入

とりあえず、上で挙げた部品を普通に買うとどのくらいするのか考えてみます。

| 品名             | 価格      | 備考               |
| :--------------- | --------: | :----------------- |
| GY-273           | 約 `¥600` | Amazon             |
| Arduino Pro Mini | `¥1,243`  | スイッチサイエンス |

うーん、予算と比べると高いですね。

## 5. プログラム作成

### 5a. HMD位置の取得

```csharp
using System;
using System.Numerics;
using System.Threading;
using OVRSharp;
using OVRSharp.Exceptions;
using OVRSharp.Math;
using Valve.VR;

namespace GetHMDLoc
{
    class Program
    {
        static void Main(string[] args)
        {
            Application app;
            try {
                app = new Application(Application.ApplicationType.Overlay);
            } catch (OpenVRSystemException<EVRInitError> e) {
                Console.Error.WriteLine("ERROR : " + e);
                return;
            }

            var data = new TrackedDevicePose_t[100];

            while (true)
            {
                app.OVRSystem.GetDeviceToAbsoluteTrackingPose(ETrackingUniverseOrigin.TrackingUniverseStanding, 0, data);

                var pose = data[0].mDeviceToAbsoluteTracking.ToMatrix4x4();

                Matrix4x4.Decompose(pose, out _, out var rotation, out var translation);
                Console.Write("translation: " + translation + "\n");
                Console.Write("rotation: " + rotation + "\n\n");
                Thread.Sleep(1000);
            }
        }
    }
}
```

```plaintext
...

translation: <-0.12683892, 0.8559739, -0.16179551>
rotation: {X:-0.063213125 Y:0.060422175 Z:0.9941291 W:0.06372233}

translation: <-0.11176507, 0.8112176, -0.15557235>
rotation: {X:0.045767996 Y:-0.013111211 Z:0.011232452 W:0.9988029}

translation: <-0.13694595, 0.77720445, -0.17874679>
rotation: {X:0.04394212 Y:-0.048361007 Z:-0.10624636 W:0.9921905}

translation: <-0.12910175, 0.76790535, -0.18021783>
rotation: {X:-0.024631226 Y:-0.6703547 Z:0.116061494 W:0.7324941}

translation: <-0.14068504, 0.7605716, -0.17483734>
rotation: {X:-0.073969394 Y:-0.72898126 Z:0.13557044 W:0.66688496}

translation: <-0.16004422, 0.77059454, -0.17738983>
rotation: {X:-0.0026735647 Y:0.2116127 Z:0.036148936 W:0.97668123}

translation: <-0.28739393, 0.7082108, -0.05348437>
rotation: {X:0.20426413 Y:-0.533753 Z:-0.59516805 W:0.5649415}

translation: <-0.26348877, 0.63101333, -0.07291168>
rotation: {X:0.47536266 Y:-0.5800788 Z:-0.42235678 W:0.50907147}

translation: <-0.26569998, 0.63390434, -0.07184511>
rotation: {X:0.47831914 Y:-0.5876847 Z:-0.4097854 W:0.5078518}

...
```
